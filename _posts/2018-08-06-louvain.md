---
title: "Test Python"
date: 2018-08-06
tags: [Machine Learning, python]
excerpt: "Test post"
comments: true
---


{% highlight python %}
import pandas as pd
import numpy as np
{% endhighlight %}

pandas: package for data frame and data manipulation<br/>
numpy: package for numerical arrays and matrices


{% highlight python %}
data_house = pd.read_csv('house_price.tsv', sep='\t')
{% endhighlight %}


```python
x = data_house.as_matrix(['size'])
y = data_house.as_matrix(['price'])
```


```python
print(x.shape, y.shape)
```

    (100, 1) (100, 1)



```python
from sklearn import linear_model
```

The sklearn.linear_model module implements generalized linear models.


```python
regr = linear_model.LinearRegression()
```

Declare regr as a linear regression model


```python
regr.fit(x, y)
```




    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)




```python
print('Coefficients:', regr.coef_)
```

    Coefficients: [[ 77.00769255]]


w_1 in y = w_1 * x_1 + w_0<br/>
Why double brackets?


```python
print('Intercept:', regr.intercept_)
```

    Intercept: [ 9161.15886434]


w_0 in y = w_1 * x_1 + w_0


```python
x = data_house.as_matrix(['size', 'Taxes'])
y = data_house.as_matrix(['price'])
```


```python
regr = linear_model.LinearRegression()
regr.fit(x, y)
```




    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)




```python
print('Coefficients:', regr.coef_)
print('Intercept:', regr.intercept_)
```

    Coefficients: [[ 34.06888701  32.12061364]]
    Intercept: [ 21115.05008427]



```python
sst = np.sum((y - np.mean(y)) ** 2)
print('Total sum of squares:', sst)
```

    Total sum of squares: 314432519600.0



```python
ssr = np.sum((regr.predict(x) - np.mean(y)) ** 2)
print('Explained sum of squares:', ssr)
```

    Explained sum of squares: 229612632471.0



```python
sse = np.sum((regr.predict(x) - y) ** 2)
print('Residual sum of squares:', sse)
```

    Residual sum of squares: 84819887129.2



```python
print('R^2 score computed from score function:', regr.score(x, y))
print('R^2 score computed from ssr / sst:', ssr / sst)
```

    R^2 score computed from score function: 0.730244545834
    R^2 score computed from ssr / sst: 0.730244545834


Is this R^2 score very good?<br/>
Any problem with this regression model?
## Let's now split the data into training data and test data


```python
from datetime import datetime
np.random.seed(datetime.now().microsecond)
```

Random variables in computer science are all pseudorandom numbers.<br/>
It is necessary to set the random seed.


```python
np.random.seed(2017)
```

For debug purposes, we may need to fix the random seed, so that bugs can be reproduced.


```python
train = np.random.choice([True, False], len(x), replace=True, p=[0.9,0.1])
```

Choose one from [True, False], for len(x) times, with probability 0.9 being True and 0.1 being False<br/>
We will put x with True sampled in the training set, with False sampled in the test set.


```python
x_train = x[train,:]
y_train = y[train]
x_test = x[~train,:]
y_test = y[~train]
```

Split x into x_train and x_test, y into y_train and y_test, with the same True/False flag


```python
regr.fit(x_train, y_train)
print('R^2 score: %.2f' % regr.score(x_test, y_test))
```

    R^2 score: 0.76


Train regr with x_train and y_train, and score are calculated based on x_test and y_test


```python
from sklearn import metrics
```


```python
y_pred = regr.predict(x_test)
print(metrics.explained_variance_score(y_test, y_pred))
print(metrics.mean_absolute_error(y_test, y_pred))
print(metrics.mean_squared_error(y_test, y_pred))
```

    0.772241913707
    18003.5638472
    533576574.336



```python
from sklearn import preprocessing

poly2 = preprocessing.PolynomialFeatures(2)
poly3 = preprocessing.PolynomialFeatures(3)

x2 = poly2.fit_transform(x)
x3 = poly3.fit_transform(x)
```


```python
x_train = x2[train,:]
x_test = x2[~train,:]
regr.fit(x_train, y_train)
print('R^2 score: %.2f' % regr.score(x_test, y_test))
```

    R^2 score: 0.82



```python
x_train = x3[train,:]
x_test = x3[~train,:]
regr.fit(x_train, y_train)
print('R^2 score: %.2f' % regr.score(x_test, y_test))
```

    R^2 score: 0.75



```python
regr_no_intercept = linear_model.LinearRegression(fit_intercept=False)

x_train = x2[train,:]
x_test = x2[~train,:]
regr.fit(x_train, y_train)
regr_no_intercept.fit(x_train, y_train)
```




    LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False)




```python
print('Coefficients:', regr.coef_)
print('Intercept:', regr.intercept_)

print('Coefficients:', regr_no_intercept.coef_)
print('Intercept:', regr_no_intercept.intercept_)
```

    Coefficients: [[  0.00000000e+00   8.47481864e+01   1.16249991e+01  -2.38283404e-02
        1.75785410e-02  -2.77655375e-03]]
    Intercept: [-1203.70443963]
    Coefficients: [[ -1.20370444e+03   8.47481864e+01   1.16249991e+01  -2.38283404e-02
        1.75785410e-02  -2.77655375e-03]]
    Intercept: 0.0


What is the difference between these two linear regression models?<br/>
## what if we'd like to have customized polynomial terms?


```python
import math

def map_to_higher_dim(orig_data, terms):
    mapped = []
    for x in orig_data:
        x_higher = []
        for d in terms:
            v = 1.0
            for pos, exponent in d.items():
                v *= math.pow(x[pos], exponent)
            x_higher.append(v)
        mapped.append(x_higher)
    return np.asarray(mapped)

terms = [{0:2}, {1:2}, {0:1,1:1}]
x_mapped = map_to_higher_dim(x, terms)
x_train = x_mapped[train,:]
x_test = x_mapped[~train,:]
regr.fit(x_train, y_train)
print('R^2 score: %.2f' % regr.score(x_test, y_test))
```

    R^2 score: 0.72



```python
from sklearn import datasets
diabetes = datasets.load_diabetes()
x = diabetes.data[:,:4]
y = diabetes.target
regr = linear_model.LinearRegression()
regr.fit(x, y)
sgd = linear_model.SGDRegressor(max_iter=100000, penalty='none')
sgd.fit(x, y)
regr.score(x, y)
sgd.score(x, y)
print(regr.coef_, regr.intercept_)
print(sgd.coef_, sgd.intercept_)
```

    [  37.24121082 -106.57751991  787.17931333  416.67377167] 152.133484163
    [  37.23702028 -106.57833515  787.17381496  416.68118645] [ 152.13051238]



```python
import matplotlib.pyplot as plt

x = diabetes.data[:,2,np.newaxis]
y = diabetes.target
regr.fit(x, y)

plt.scatter(x, y, color = 'red')
lx = np.arange(min(x), max(x), (max(x) - min(x)) / 200).reshape(200, 1)
plt.plot(lx, regr.predict(lx), color='blue', linewidth = 3)
plt.show()
```

![png]({{"/images/test/output_42_0.png"}})




```python
from mpl_toolkits.mplot3d import Axes3D

x = diabetes.data[:,[2,8]]
y = diabetes.target
regr = linear_model.LinearRegression()
regr.fit(x, y)
steps = 40
lx0 = np.arange(min(x[:,0]), max(x[:,0]), (max(x[:,0]) - min(x[:,0])) / steps).reshape(steps,1)
lx1 = np.arange(min(x[:,1]), max(x[:,1]), (max(x[:,1]) - min(x[:,1])) / steps).reshape(steps,1)
xx0, xx1 = np.meshgrid(lx0, lx1)
xx = np.zeros(shape = (steps,steps,2))
xx[:,:,0] = xx0
xx[:,:,1] = xx1
x_stack = xx.reshape(steps ** 2, 2)
y_stack = regr.predict(x_stack)
yy = y_stack.reshape(steps, steps)

fig = plt.figure()
ax = fig.gca(projection = '3d')
ax.scatter(x[:,0], x[:,1], y, color = 'red')
ax.plot_surface(xx0, xx1, yy, rstride=1, cstride=1)
plt.show()
```


![png]({{"/images/test/output_43_0.png"}})
