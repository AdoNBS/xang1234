{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel Classification with scikit-multilearn\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "We typically group supervised machine learning problems into classification and regression problems. Within the classification problems sometimes, multiclass classification models are encountered where the classification is not binary but we have to assign a class from `n` choices. \n",
    "\n",
    "In multilabel classification, instead of one target variable $y$, we have multiple target variables $y_1$, $y_2$, ..., $y_n$. For example there can be multiple objects in an image and we need to correctly classify them all or we are attempting predict which combination of a product that a customer would buy. \n",
    "\n",
    "Certain decision tree based algorithms in [Scikit-Learn](http://scikit-learn.org/stable/modules/multiclass.html) are naturally able to handle multilabel classification. [scikit-multilearn](http://scikit.ml/) leverages Scikit-Learn and is built specifically for multilabel problems. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Datasets\n",
    "\n",
    "We use the AmazonCat-13K dataset to explore different multilabel algorithms available in Scikit-Multilearn. Our goal is not to optimize classifier performance but to explore the various algorithms applicable to multilabel classification problems. The dataset if fairly large with over 1.1M train points and 300k test points. There are over 200k features and 13k labels. \n",
    "\n",
    "This dataset was chosen in order to work with a fairly large dataset to illustrate difficulties in multilabel classification instead of a toy example. In particular when there are $N$ labels, the search space increases exponentially to $2^N$. A list of multilabel datasets can be found at Manik Varma's [Extreme Classification Repository](http://manikvarma.org/downloads/XC/XMLRepository.html). The data is provided in sparse format and the authors only provide Matlab scripts to convert them; some data wrangling is needed in python to handle them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "import gc\n",
    "\n",
    "f = open(r'C:\\Users\\david\\Downloads\\RCV1-x\\rcv1x_train.txt', 'r',encoding='utf-8')\n",
    "\n",
    "size = f.readline()\n",
    "nrows, nfeature,nlabel = [int(s) for s in size.split()]\n",
    "x_m = [[] for i in range(nrows)]\n",
    "pos = [[] for i in range(nrows)]\n",
    "y_m = [[] for i in range(nrows)]\n",
    "\n",
    "for i in range(nrows):\n",
    "    line = f.readline()\n",
    "    temp=[s for s in line.split(sep=' ')]\n",
    "    pos[i]=[int(s.split(':')[0]) for s in temp[1:]]\n",
    "    x_m[i]=[float(s.split(':')[1]) for s in temp[1:]]\n",
    "    for s in temp[0].split(','):\n",
    "        try:\n",
    "            int(s)\n",
    "            y_m[i]=[ int(s) for s in temp[0].split(',')]\n",
    "        except:\n",
    "            y_m[i]=[]\n",
    "          \n",
    "\n",
    "x_train=sparse.lil_matrix((nrows,nfeature))\n",
    "for i in range(nrows):\n",
    "    for j in range(len(pos[i])):\n",
    "        x_train[i,pos[i][j]]=x_m[i][j]\n",
    "\n",
    "del x_m,pos\n",
    "gc.collect()\n",
    "\n",
    "y_train=sparse.lil_matrix((nrows,nlabel))\n",
    "for i in range(nrows):\n",
    "    for j in y_m[i]:\n",
    "        y_train[i,j]=1\n",
    "\n",
    "del y_m\n",
    "gc.collect()  \n",
    "\n",
    "\n",
    "f = open(r'C:\\Users\\david\\Downloads\\RCV1-x\\rcv1x_test.txt', 'r',encoding='utf-8')\n",
    "\n",
    "size = f.readline()\n",
    "nrows, nfeature,nlabel = [int(s) for s in size.split()]\n",
    "x_m = [[] for i in range(nrows)]\n",
    "pos = [[] for i in range(nrows)]\n",
    "y_m = [[] for i in range(nrows)]\n",
    "\n",
    "for i in range(nrows):\n",
    "    line = f.readline()\n",
    "    temp=[s for s in line.split(sep=' ')]\n",
    "    pos[i]=[int(s.split(':')[0]) for s in temp[1:]]\n",
    "    x_m[i]=[float(s.split(':')[1]) for s in temp[1:]]\n",
    "    for s in temp[0].split(','):\n",
    "        try:\n",
    "            int(s)\n",
    "            y_m[i]=[ int(s) for s in temp[0].split(',')]\n",
    "        except:\n",
    "            y_m[i]=[]\n",
    "          \n",
    "\n",
    "x_test=sparse.lil_matrix((nrows,nfeature))\n",
    "for i in range(nrows):\n",
    "    for j in range(len(pos[i])):\n",
    "        x_test[i,pos[i][j]]=x_m[i][j]\n",
    "\n",
    "del x_m,pos\n",
    "gc.collect()\n",
    "\n",
    "y_test=sparse.lil_matrix((nrows,nlabel))\n",
    "for i in range(nrows):\n",
    "    for j in y_m[i]:\n",
    "        y_test[i,j]=1\n",
    "\n",
    "del y_m\n",
    "gc.collect()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Metric  \n",
    "\n",
    "Before going into the details of each multilabel classification method, we select a metric to gauge how well the algorithm is performing. Similar to a classification problem it is possible to use `Hamming Loss`, `Accuracy`, `Precision`, `Jaccard Similarity`, `Recall`, and `F1 Score`. These are available from Scikit-Learn. \n",
    "\n",
    "Going forward we'll chose the `F1 Score` as it averages both `Precision` and `Recall`. It is also helpful to plot the confusion matrix to understand how the classifier is performing, but in our case there are too many labels to visualize.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-853b10b3eedc>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-853b10b3eedc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Image of confusion matrix\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Image of confusion matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Problem Transformation\n",
    "\n",
    "#### Binary Relevance \n",
    "\n",
    "Binary relevance is simple; each target variable ($y_1$, $y_2$,..,$y_n$) is treated independently and we are reduced to $n$ classification problems. `Scikit-Multilearn` implements this for us, saving us the hassle of splitting the dataset and training each of them separately.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "\n",
    "classifier = BinaryRelevance(\n",
    "    classifier = RandomForestClassifier(),\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "start=time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_hat = classifier.predict(x_test)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "metrics.f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Powerset\n",
    "\n",
    "This method transforms the problem into a multiclass classification problem; the target variables ($y_1$, $y_2$,..,$y_n$) are combined and each combination is treated as a unique class. This method will produce many classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "classifier = ClassifierChain(\n",
    "    classifier = RandomForestClassifier(),\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "start=time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_hat = classifier.predict(x_test)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Chains \n",
    "\n",
    "Classifier chains are akin to binary relevance, however the target variables ($y_1$, $y_2$,..,$y_n$) are not fully independent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "\n",
    "classifier = ClassifierChain(\n",
    "    classifier = RandomForestClassifier,\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "start=time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_hat = classifier.predict(x_test)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
