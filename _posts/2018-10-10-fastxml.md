---
title: "Extreme Multi-label classification - FastXML"
date: 2018-10-10
tags: [Machine Learning, Classification, multi-label, FastXML]
excerpt: ""
mathjax: "true"
comments: true
header:
  teaser: images/underconstruction.jpg
---

<img src="{{site.url }}{{site.baseurl }}/images/underconstruction.jpg" alt="">

### 1. Introduction

Previously we [explored]({{site.url }}{{site.baseurl }}/multi-label) multi-label algorithms using the [Scikit-multilearn](http://scikit.ml/)


link to ranking, search engine



### 2. Datasets

Datasets are

### 3. Metrics - Normalized Discounted Cummulative Gain (nDCG)

The notion of what constitutes a good prediction changes when we go for a few labels to millions of labels. The number of relevant/positive labels for any data point is significantly smaller than the set of possible labels. As such, typical multi-label metrics that we have used such as F1-score and Hamming loss would give equal weight to positive and negative labels. It is more important for us to correctly predict the positive labels vs the negative labels.

To favor positive labels we use ranking based measures. For example, precision at $$k$$ only focuses on how many of the top k positive labels are predicted. It is thus sensitive to the relevance of the prediction. To incorporate a measure of the ranking quality, we need a rank sensitive loss function and in our case, the [normalized discounted cumulative gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) is used.

In our multi-label case we have the data points $$\{(x_i,y_i)_i=1^N\}$$ where $$x_i \in \Bbb{R}^D$$ are $$D$$ dimensional real feature vectors and $$y_i \in {0,1}^L$$ are the L dimensional binary label vectors ($$y_il=1$$ if the label l is relevant for point $$i$$ )





### 4. FastXML

An efficient methode that is able to scale to millions of labels is the FastXML algorithm by Yashoteja Prabhu and Manik Varma as described in their
[paper](https://www.microsoft.com/en-us/research/publication/fastxml-a-fast-accurate-and-stable-tree-classifier-for-extreme-multi-label-learning/).

FastXML is a tree based classifier algorithm that partitions the feature space instead of the label space. The intuition is that there are only a few labels associated with a region of the feature space. At any region of the feature space, the set of active labels are the union of the labels of all the region's training points. At each node, the parent feature space is partitioned into a left and right child space. In decision trees, [Gini impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity),
[Information gain](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees) or clustering error is typically used to split each node. As these measures are not suited for extreme multi-label applications, the authors have proposed to use nDCG instead as it is a ranking loss function. This will recommend better labels as relevant positive labels with the highest rank are returned. nDCG is optimized accross all $$L$$ labels
at the current node;


Refefer implemented the FastXML algorithm in python/cython as share in [this github repository](https://github.com/Refefer/fastxml/tree/master/fastxml)
### 5. Performance on Datasets
